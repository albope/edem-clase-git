{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EDEM - Text Generation (Practice 3)",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albope/master-data-analytics-content/blob/master/EDEM_Text_Generation_(Practice_3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA_RQDsTRV8J",
        "colab_type": "text"
      },
      "source": [
        "# Text generation with an RNN\n",
        "Based on https://www.tensorflow.org/tutorials/text/text_generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iOEa_V6Rwkz",
        "colab_type": "text"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t56-8zhRKXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUaZJreWR2a8",
        "colab_type": "text"
      },
      "source": [
        "### Download BBC News dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rJB6A90R1zd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkYoU2ZqSjzo",
        "colab_type": "text"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXBnor9ySPxd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1037cd00-cc76-49cd-de05-9658e982e415"
      },
      "source": [
        "len(text)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7RheV6aSwga",
        "colab_type": "code",
        "outputId": "73ed9011-dbff-41b5-863a-36c816c08e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:1000])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrOfDrm1rFaL",
        "colab_type": "code",
        "outputId": "15fe2c9e-ac3f-4623-a138-4b30c9f236fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(text)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfjnuEcUS1yM",
        "colab_type": "code",
        "outputId": "3ecac948-3d04-4631-8933-61d40d606484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text)) #toma el texto y lo convierte en un set (es decir crea un conjunto que no tiene elementos repetidos)\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-9RkcI0TUTa",
        "colab_type": "text"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq27MQJET_WO",
        "colab_type": "text"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQHZnq4NT5oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLoWiUSDTTfD",
        "colab_type": "code",
        "outputId": "48ccdd4a-bb98-477a-f244-10ee626b4be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd414WrhVlcZ",
        "colab_type": "text"
      },
      "source": [
        "### Create training examples and targets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yigYZs3XVMXB",
        "colab_type": "code",
        "outputId": "cdccfd97-b049-44ff-a212-b3a6c6092b0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIGQrKP0Zbba",
        "colab_type": "code",
        "outputId": "2510900a-e007-4ab1-f6fc-eee1b79f0193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWI6ND9LY0TZ",
        "colab_type": "text"
      },
      "source": [
        "Given a character, or a sequence of characters, what is the most probable next character?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaBflgP-YU2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL7N43PRZ2FK",
        "colab_type": "code",
        "outputId": "d05dd2af-06a7-41b6-b5d2-15584671994a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c60SmABWampM",
        "colab_type": "code",
        "outputId": "18e3ccf9-f481-492c-8419-83a49a5daf8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6fTL3-LamnP",
        "colab_type": "code",
        "outputId": "43b448e2-c70b-4863-b242-76d6c6902ff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fl2B_GAbrAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djOwLWTHb-Zh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9H_kCp8cPt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziBntd0-dQ_7",
        "colab_type": "text"
      },
      "source": [
        "## Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj5Z26TQdL-y",
        "colab_type": "code",
        "outputId": "97c2b293-b44e-4f43-b177-2d995f31be1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHaFC5xodebz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l72tZl-bdiQV",
        "colab_type": "code",
        "outputId": "aea83aec-1334-425a-8d95-7a6d0ef19b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([20, 15, 19,  0, 23,  7, 30, 57, 47, 13, 36, 62,  1, 28, 40, 25, 33,\n",
              "       29, 44, 57, 27, 20, 25, 17, 52,  0,  7, 15, 39,  0, 56, 41, 49, 22,\n",
              "       29, 57, 64, 43, 51, 51,  9,  5,  9, 45, 64, 48, 43, 53, 51, 42, 53,\n",
              "        9,  5,  2, 53, 17, 38, 49, 47, 63,  9, 28, 43, 54, 45, 51, 58, 40,\n",
              "       22, 58, 50, 51, 40,  5, 40,  6,  4,  6, 26, 31, 25, 44, 48, 34, 10,\n",
              "       10, 52, 33, 15, 57,  5, 31, 40, 55, 16, 27, 10, 39, 30,  5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq6S16svdl4v",
        "colab_type": "code",
        "outputId": "ecec3531-ed48-478d-c50a-c93528a646c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " \"hy blood,\\nCongeal'd with this, do make me wipe off both.\\n3 KING HENRY VI\\n\\nYORK:\\nThe army of the quee\"\n",
            "\n",
            "Next Char Predictions: \n",
            " \"HCG\\nK-RsiAXx PbMUQfsOHMEn\\n-Ca\\nrckJQszemm3'3gzjeomdo3'!oEZkiy3PepgmtbJtlmb'b,&,NSMfjV::nUCs'SbqDO:aR'\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CpEpe3IejBJ",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiB-Y0l_ee1F",
        "colab_type": "code",
        "outputId": "34bb2f19-7336-48ae-ec5b-5750d596a37c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.17468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W3ZH6bwepDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP2yJ8JxetyA",
        "colab_type": "text"
      },
      "source": [
        "Configure checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWrwaCTOes1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmYh3Qr9fjz7",
        "colab_type": "text"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTnbxIbjfoWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wL7msCofhxf",
        "colab_type": "code",
        "outputId": "46ce43fe-b737-4af8-c850-f95b03c668d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 172 steps\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 12s 69ms/step - loss: 2.8311\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 11s 63ms/step - loss: 2.1087\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 11s 64ms/step - loss: 1.8331\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 11s 64ms/step - loss: 1.6602\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 11s 64ms/step - loss: 1.5482\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 11s 64ms/step - loss: 1.4716\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 11s 65ms/step - loss: 1.4153\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 1.3722\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 11s 65ms/step - loss: 1.3357\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 11s 65ms/step - loss: 1.3046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfuZfX0Pgk2Y",
        "colab_type": "text"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0KJ1R_BgphP",
        "colab_type": "text"
      },
      "source": [
        "Restore the latest checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWlR4gocgcYG",
        "colab_type": "code",
        "outputId": "4e6d8cc3-ccdc-46c9-b9f2-23068d044786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JrVSzLGgv_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBciyleIh3Ez",
        "colab_type": "text"
      },
      "source": [
        "Prediction loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRI4M2XCgzMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 2.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDZgLeoLiXzt",
        "colab_type": "code",
        "outputId": "3c3a313a-f43b-465e-ded7-ad02908b7ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"yesterday \"))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yesterday teg therdeyoure\n",
            "ishofowitik' t fusinoro towhont pathofalllll or'\n",
            "\n",
            "\n",
            "Canieareelinom avamses, thaini's tofige were'llut l buestake,\n",
            "\n",
            "AS: lley mererme\n",
            "\n",
            "Su vanco's thisouthampreelie dosoisenoftausownthill ure Prerdowinde ir ou motan, chaiGHeshert teniomeseare\n",
            "\n",
            "Th\n",
            "Shi'ldwourevan s I mouso wADK\n",
            "ST:\n",
            "Chorofas yous\n",
            "TEd blllfe alour f s topp icyopllt ya-plll medon Is the RWh s f oul y t bre iranofororra byot gedoullire, ivim soualkig!-Phes a quiouhed d!\n",
            "Yobul s clld, sougr teesod\n",
            "INRGXFVInif inofr'st WAURINThioueichee yoneisel, orelllirlllladashed AN meng f m he t, ang t\n",
            "Finge d! w me bun\n",
            "Towin thondollll her by th, tir I'prhene g get d s I ain or:\n",
            "S:\n",
            "An burey maknoug aye pend, morstheve bace Time,\n",
            "ARYould s!\n",
            "NGRI'st. Y:\n",
            "Anouseia g p n:\n",
            "Anive.\n",
            "\n",
            "SThelomy sonoaje ichounean g gevee mamy;\n",
            "ANGRLAndeoutoul y mo th herise,\n",
            "NThayowest othe an:\n",
            "Whano cen-\n",
            "Socke outhamy thanoma the y pavey pe onghongounges ty wore'POMed?\n",
            "\n",
            "TRUlour hyonaieidy ne?'laifsan\n",
            "Esee ithereala.\n",
            "Thio lontiethe brglfus\n",
            "INsuyives best,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAmqe7JjuaDx",
        "colab_type": "code",
        "outputId": "2bb15ee6-1ba6-4b69-c58c-4c227fa3353a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"before\"))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before!\n",
            "INoume eer th s han his brt giny trbe, o'semanat mar serd IO geio 'r st GOUS m omo. ooryo ilingl\n",
            "\n",
            "THonounyat estived,\n",
            "\n",
            "\n",
            "LAn, ontso pravall fout, s thant orerevifan m'flletons\n",
            "ENo w'\n",
            "S:\n",
            "RDSWr\n",
            "OMED wakil the wheret.\n",
            "STrealomousen;\n",
            "\n",
            "BERINClacee al icurath MERAnirdrer hilofr he! JUSTHe be furenous nicin hy inout makeset, me ofothe s, twindsorant h herocouse bo tis,\n",
            "\n",
            "WIENThen h'pe\n",
            "WARINCho LAPeo afowhar y Pu, tort sthey.\n",
            "IORGRAnove ate ther d merot soriceeth am wachisheat IERCHer I's nd blufishe, o there ange honitl th br\n",
            "Mrs s; t, I\n",
            "Fowatof s, tam hom,\n",
            "VIO:\n",
            "\n",
            "STHAD\n",
            "Prellly, s, atinor?\n",
            "'ZARK:\n",
            "Shan see ser'D ath thed sap'GHasimethang iomey we:\n",
            "S: w d and sofoconobe!\n",
            "\n",
            "Yo anodadullinous, atu altis orir am tste Vesid; wise figas h!\n",
            "'fre the\n",
            "\n",
            "ABARUSt.\n",
            "He, pr me sofu gg the,\n",
            "\n",
            "Wh thy ting h d blinghesoouta th tha,\n",
            "He IO:\n",
            "BTRULIStheratheshouks hay ho bhey howonge ain icou fore he benkinouprbe bo my l t ot;\n",
            "IOLUCARD yourouatof wntis ith acouscofin? sll werinoucaton inochtsed by athans\n",
            "Th h oo; cong\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncvnui1qiprX",
        "colab_type": "text"
      },
      "source": [
        "# Exercises\n",
        "1. Create a new colab Notebook\n",
        "2. Create a Shakespeare generator\n",
        "3. Use different configuration of RNN (BiLSTM, Stacked LSTM)\n",
        "4. Train the model for 10 epochs\n",
        "5. Print some results\n",
        "6. (Optional) Try traning with words instead of chars\n",
        "\n",
        "## Practical tips\n",
        "\n",
        "Download the data using: \n",
        "\n",
        "```\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "```\n",
        "\n"
      ]
    }
  ]
}